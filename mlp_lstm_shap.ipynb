{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce203f32",
   "metadata": {},
   "source": [
    "### Notebook creado por **Guillermo Grande Santi**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e3dde7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4802c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 19:36:50.678836: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746034610.743306   24554 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746034610.771094   24554 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746034610.928675   24554 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746034610.928703   24554 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746034610.928704   24554 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746034610.928705   24554 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-30 19:36:50.959461: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/ggs/Fake-News-Detection/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import math\n",
    "from collections import Counter\n",
    "# from scipy.stats import norm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential # type: ignore\n",
    "from tensorflow.keras.layers import LSTM, Dense # type: ignore\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences # type: ignore\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer # type: ignore\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "import contractions\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b8e51a",
   "metadata": {},
   "source": [
    "# Explicabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbd9173",
   "metadata": {},
   "source": [
    "### Modelo con Tensorflow para facilitar explicabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9068e3",
   "metadata": {},
   "source": [
    "En un inicio, no se ha empleado **TensorFlow** para entrenar los modelos por dos motivos principales:\n",
    "\n",
    "1. Empleamos TensorFlow 2.19.0 junto con CUDA 12.1, pero a partir de la versión 2.11 el paquete oficial para Windows dejó de incluir **soporte GPU** (el último que lo integra de forma nativa es el 2.10, que requiere CUDA 11.2).\n",
    "\n",
    "2. Durante el Máster, en asignaturas como *Redes Neuronales* y *Aprendizaje Profundo*, trabajamos habitualmente con **PyTorch**, lo cual resultó más ágil y familiar y se prefirió por encima de cambiar versiones de CUDA y Tensorflow.\n",
    "\n",
    "Sin embargo, muchos métodos de explicabilidad están diseñados para TensorFlow y la definición y el entrenamiento de modelos en Keras suelen ser más sencillos. Por ello, a continuación migraremos nuestro mejor modelo de PyTorch a TensorFlow. Además, hemos levantado una máquina virtual Debian bajo WSL2 para aprovechar la GPU y reducir drásticamente los tiempos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77cf73c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbefc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746034630.794990   24554 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5592 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:05:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Cargar el DataFrame limpio\n",
    "df = pd.read_csv(\"Datasets/Cleaned-FR-News_V2.csv\")\n",
    "\n",
    "# Dividimos los datos en entrenamiento y prueba\n",
    "# Por ahora usaremos únicamente el texto de la noticia (omitimos el título)\n",
    "X = df[\"clean_text\"]\n",
    "y = df[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Se usará para redes neuronales\n",
    "# Usaremos un 20% del conjunto de datos para validación (16% del total)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "def create_dataset(train, test, shuffle=True):\n",
    "    # Create a TensorFlow dataset from the text and fake columns of the dataframe\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((train, test))\n",
    "    if shuffle:\n",
    "         # Shuffle the dataset if the shuffle parameter is True\n",
    "        dataset = dataset.shuffle(1024, reshuffle_each_iteration=True)\n",
    "    # Batch the dataset into smaller batches of size 256\n",
    "    dataset = dataset.batch(256).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    # Prefetch the next batch of data to further optimize training\n",
    "    return dataset\n",
    "\n",
    "train_ds = create_dataset(X_train, y_train)\n",
    "valid_ds = create_dataset(X_valid, y_valid, shuffle=False)\n",
    "test_ds = create_dataset(X_test, y_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1241444b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 19:37:13.619942: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 104327280 exceeds 10% of free system memory.\n",
      "2025-04-30 19:37:13.620009: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 156490920 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "# Create a TextVectorization layer with specified parameters\n",
    "vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=10000, \n",
    "    output_sequence_length=1024, \n",
    "    pad_to_max_tokens=True\n",
    ")\n",
    "# Adapt the TextVectorization layer to the training data\n",
    "vectorizer.adapt(X_train, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab87d642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ggs/Fake-News-Detection/.venv/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">640,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,216</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,040</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTextVectorization\u001b[0m)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │       \u001b[38;5;34m640,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m66,048\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m41,216\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │         \u001b[38;5;34m1,040\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">748,321</span> (2.85 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m748,321\u001b[0m (2.85 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">748,321</span> (2.85 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m748,321\u001b[0m (2.85 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(), dtype=tf.string),\n",
    "    vectorizer,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=10000, \n",
    "        output_dim=64,\n",
    "        input_length=1024, \n",
    "        mask_zero=True\n",
    "    ),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)), \n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)), \n",
    "    tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(), \n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), \n",
    "    metrics=[\n",
    "        \"accuracy\", \n",
    "        tf.keras.metrics.AUC(name=\"auc\")\n",
    "    ]\n",
    ")\n",
    "model.summary()\n",
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7488a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asignación de ops en dispositivos:\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746034650.677802   24673 cuda_dnn.cc:529] Loaded cuDNN version 90501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 375ms/step - accuracy: 0.8834 - auc: 0.9362 - loss: 0.3759 - val_accuracy: 0.9727 - val_auc: 0.9963 - val_loss: 0.0867\n",
      "Epoch 2/10\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 376ms/step - accuracy: 0.9831 - auc: 0.9980 - loss: 0.0555 - val_accuracy: 0.9877 - val_auc: 0.9985 - val_loss: 0.0396\n",
      "Epoch 3/10\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 366ms/step - accuracy: 0.9954 - auc: 0.9996 - loss: 0.0163 - val_accuracy: 0.9884 - val_auc: 0.9979 - val_loss: 0.0415\n",
      "Epoch 4/10\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 360ms/step - accuracy: 0.9987 - auc: 0.9999 - loss: 0.0070 - val_accuracy: 0.9884 - val_auc: 0.9966 - val_loss: 0.0445\n",
      "Epoch 5/10\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 362ms/step - accuracy: 0.9992 - auc: 0.9999 - loss: 0.0047 - val_accuracy: 0.9868 - val_auc: 0.9974 - val_loss: 0.0415\n",
      "Epoch 6/10\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 371ms/step - accuracy: 0.9997 - auc: 0.9999 - loss: 0.0028 - val_accuracy: 0.9884 - val_auc: 0.9981 - val_loss: 0.0368\n",
      "Epoch 7/10\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 355ms/step - accuracy: 0.9998 - auc: 0.9999 - loss: 0.0017 - val_accuracy: 0.9890 - val_auc: 0.9976 - val_loss: 0.0389\n",
      "Epoch 8/10\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 293ms/step - accuracy: 0.9999 - auc: 0.9999 - loss: 0.0012 - val_accuracy: 0.9898 - val_auc: 0.9975 - val_loss: 0.0375\n",
      "Epoch 9/10\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 338ms/step - accuracy: 0.9999 - auc: 1.0000 - loss: 8.7454e-04 - val_accuracy: 0.9881 - val_auc: 0.9977 - val_loss: 0.0422\n",
      "Epoch 10/10\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 385ms/step - accuracy: 0.9999 - auc: 1.0000 - loss: 7.6253e-04 - val_accuracy: 0.9885 - val_auc: 0.9967 - val_loss: 0.0463\n"
     ]
    }
   ],
   "source": [
    "# Verificación rápida\n",
    "import tensorflow as tf\n",
    "print(\"Asignación de ops en dispositivos:\")\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "file_path = \"models/best_bilstm.keras\"\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=10, \n",
    "    validation_data=valid_ds,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            file_path,\n",
    "            save_best_only=True,\n",
    "            monitor='val_accuracy',\n",
    "            mode='max'\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b98e34",
   "metadata": {},
   "source": [
    "Consideramos óptimo el modelo que alcanza el máximo valor de *val_accuracy* durante el entrenamiento. En este experimento, la mejor convergencia se produce en la segunda época, con un val_accuracy del **99,85 %** y una pérdida de **0,0396**, lo que supone un ligero progreso respecto al modelo en PyTorch. A continuación, presentamos los resultados obtenidos sobre el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4b4e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 0.9911 - auc: 0.9986 - loss: 0.0295\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the TensorFlow model using the test_ds dataset\n",
    "model = tf.keras.models.load_model(file_path)\n",
    "eval_results = model.evaluate(test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bca830",
   "metadata": {},
   "source": [
    "En el conjunto de prueba, el modelo alcanza una precisión de **99,11 %**, el mejor resultado obtenido hasta la fecha. El área bajo la curva ROC —que mide la capacidad del modelo para distinguir correctamente entre clases positivas y negativas en todos los umbrales posibles— es de **99,86 %**, y la pérdida final se sitúa en **0,0295**.\n",
    "\n",
    "Es posible que la pequeña mejora (de aproximadamente un **0,2 %** respecto al modelo en PyTorch) se deba a una mejor semilla de inicialización y a utilizar un tamaño de *batch* mayor, que en este caso fue de 256.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283e03f0",
   "metadata": {},
   "source": [
    "### Explicabilidad mediante Deep SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc66e2a",
   "metadata": {},
   "source": [
    "El método **Deep Explainer** de SHAP no puede utilizar los 28.000 datos de entrenamiento completos debido a las limitaciones computacionales al calcular los **valores SHAP para cada muestra** en un modelo complejo como **BiLSTM** (El proceso requiere evaluar el modelo en múltiples combinaciones de características).\n",
    "\n",
    "Por esta razón, se seleccionan **muestras de fondo** (background samples), para aproximar el valor esperado de la salida del modelo (E[f(x)]). La aproximación se basa en la idea de que la diferencia entre la salida del modelo para una muestra específica f(x) y el valor esperado E[f(x)] (promedio de las salidas sobre las muestras de fondo) refleja cómo una característica en particular contribuye a la predicción para esa muestra. El uso de estas muestras permite realizar estimaciones eficientes de los valores SHAP **sin sacrificar la calidad de las explicaciones**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c70fc3",
   "metadata": {},
   "source": [
    "### Explicabilidad mediante Integrated Gradients"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
